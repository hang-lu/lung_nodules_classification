{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split,StratifiedShuffleSplit\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score,roc_auc_score,accuracy_score\n",
    "from tqdm import tqdm \n",
    "import json \n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from copy import deepcopy,copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annos = json.load(open('./data_new/annos.json','r'))\n",
    "keys=['label', 'subtlety', 'internalStructure', 'calcification', 'sphericity', 'margin', 'lobulation', 'spiculation', 'texture']\n",
    "# keys2id =[5, 9, 15, 20, 25, 30, 35, 40]\n",
    "# keys2id ={ 'subtlety': 5, 'internalStructure': 4, 'calcification': 6, 'sphericity': 5, 'margin': 5, 'lobulation': 5, 'spiculation': 5, 'texture': 5}\n",
    "keys2id ={ 'subtlety': 0, 'internalStructure': 5, \n",
    "          'calcification': 9, 'sphericity': 15, \n",
    "          'margin': 20, 'lobulation': 25, \n",
    "          'spiculation': 30, 'texture': 35\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = np.zeros((len(annos),40),dtype=int)\n",
    "for lung in annos:\n",
    "    row_id = int(lung)-1\n",
    "    for key in annos[lung]:\n",
    "        if key =='label':\n",
    "            continue\n",
    "        idx= keys2id[key]+ annos[lung][key]-1\n",
    "        H[row_id][idx] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.sparse.linalg\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "\n",
    "def getBase(H_dense):\n",
    "    K = 500\n",
    "    tolerant = 0.1 ** 5\n",
    "    epsilon = 0.1 ** 10\n",
    "    patientNum, featureNum = len(H_dense), len(H_dense[0])\n",
    "    H = sp.sparse.lil_matrix((patientNum, featureNum))\n",
    "    D = sp.sparse.lil_matrix((patientNum, patientNum))\n",
    "    D_edge = sp.sparse.lil_matrix((featureNum, featureNum))\n",
    "    I = sp.sparse.lil_matrix(np.eye(patientNum, patientNum))\n",
    "\n",
    "    # constructing the laplacian matrices\n",
    "    print('Constructing the laplacian matrices...')\n",
    "    for patient in range(patientNum):\n",
    "        for feature in range(featureNum):\n",
    "            if H_dense[patient][feature] > 0:\n",
    "                H[patient, feature] = H_dense[patient][feature]\n",
    "                D[patient, patient] += H_dense[patient][feature]\n",
    "                D_edge[feature, feature] += H_dense[patient][feature]\n",
    "    # print(H)\n",
    "    D_n = sp.sparse.lil_matrix((patientNum, patientNum))\n",
    "    D_e = sp.sparse.lil_matrix((featureNum, featureNum))\n",
    "    for i in range(patientNum):\n",
    "        D_n[i, i] = 1.0 / max(sqrt(D[i, i]), epsilon)\n",
    "    for i in range(featureNum):\n",
    "        D_e[i, i] = 1.0 / max(D_edge[i, i], epsilon)\n",
    "    L = I - D_n * H * D_e * H.T * D_n\n",
    "\n",
    "    #eigenvalue factorization\n",
    "    print('Decomposing the laplacian matrices...')\n",
    "    [Lamda, P] = sp.sparse.linalg.eigsh(L, k = K, which='SM', tol = tolerant)\n",
    "    print(Lamda[0:10])\n",
    "    return(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing the laplacian matrices...\n",
      "Decomposing the laplacian matrices...\n",
      "[1.41457433e-15 6.79548585e-01 7.31341624e-01 8.12738063e-01\n",
      " 8.28526251e-01 8.39241781e-01 8.42142157e-01 8.46591347e-01\n",
      " 8.50461969e-01 8.62346452e-01]\n"
     ]
    }
   ],
   "source": [
    "P=getBase(H.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络层\n",
    "class GCNLayer(torch.nn.Module):\n",
    "    def __init__(self,k_size,P,device):\n",
    "        super().__init__()\n",
    "        self.P=torch.tensor(P[:, 0: k_size]).double().to(device)\n",
    "        self.k_size=k_size\n",
    "        self.k =Parameter(torch.Tensor(k_size))\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.k.unsqueeze(0),a=math.sqrt(5))\n",
    "    def forward(self,x):\n",
    "        return self.P.mm(self.k.diag().mm(self.P.t().mm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import math\n",
    "\n",
    "class gcn(torch.nn.Module):\n",
    "    def __init__(self,in_features, out_features,k_size,layers ,P,device,alpha=[1,0.1,0.01,0.001]):\n",
    "        super().__init__()\n",
    "        self.layers= layers\n",
    "        self.in_features=in_features\n",
    "        self.out_features=out_features\n",
    "        self.k_size=k_size\n",
    "#         self.linear = nn.Linear(self.in_features,self.out_features*8)\n",
    "        self.embeddings = nn.Embedding(self.in_features,self.out_features)\n",
    "        self.gcnlayers = nn.ModuleList()\n",
    "        for layer in range (self.layers):\n",
    "            self.gcnlayers.append(GCNLayer(self.k_size,P,device))\n",
    "        self.device = device\n",
    "#         self.fc = nn.Linear((2048+self.out_features*8)*(self.layers+1),2)\n",
    "        self.fc = nn.Linear((2048+self.out_features*8),2)\n",
    "        self.alpha = alpha \n",
    "    def forward(self,x1,x2):\n",
    "#         t_features= self.linear(x1)\n",
    "        t_features= self.embeddings(x1)\n",
    "        t_features= torch.reshape(t_features,(t_features.shape[0],-1))\n",
    "        out= torch.cat((x2,t_features),dim=1)\n",
    "        outputs =[out]\n",
    "        \n",
    "        for i,gcnlayer in enumerate(self.gcnlayers):\n",
    "            out = torch.relu(gcnlayer(out))\n",
    "#             out = out + self.alpha[i]*torch.relu(gcnlayer(out))\n",
    "#             outputs += self.alpha[i] * out \n",
    "            outputs.append(out)\n",
    "            \n",
    "        out = sum([outputs[i]*self.alpha[i] for i in range(self.layers+1)])\n",
    "        return (self.fc(out))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config \n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "max_epoch=500\n",
    "out_dir = 'result_out_gcn'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "best_accuracy = 1e-9\n",
    "val_interval=1\n",
    "# weight_decay=0.01\n",
    "# gcn_layer_num = 1\n",
    "# k_size = 200\n",
    "# learning_rate=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label \n",
    "np_dir = './data'\n",
    "random_state = 0\n",
    "nps_labels = np.array([[os.path.join(np_dir, '{}.npy'.format(fig_id)), annos[fig_id]['label']] for fig_id in annos])\n",
    "sss= StratifiedShuffleSplit(n_splits=1,test_size=0.3,random_state=random_state)\n",
    "# train_index,val_index=next((sss.split(nps_labels,nps_labels[:,1])))\n",
    "train_index,val_index=next((sss.split(nps_labels,nps_labels[:,1])))\n",
    "# train_nps,val_nps=nps_labels[train_index,0],nps_labels[val_index,0]\n",
    "# train_labels,val_labels= nps_labels[train_index,1].astype(np.int),nps_labels[val_index,1].astype(np.int)\n",
    "train_val_labels=torch.from_numpy(nps_labels[:,1].astype(np.int)).long().to(device)\n",
    "\n",
    "train_val_labels[train_val_labels==-1]=0\n",
    "train_flag = torch.zeros(len(nps_labels)).long()\n",
    "train_flag[train_index]=1\n",
    "\n",
    "# data\n",
    "x2= np.load('./features.npy')\n",
    "torch_x2= torch.from_numpy(x2).double().to(device)\n",
    "h=[]\n",
    "for row in H:\n",
    "    h.append([])\n",
    "    for i in range(H.shape[1]):\n",
    "        if row[i]:\n",
    "            h[-1].append(i)\n",
    "torch_x1= torch.LongTensor(h).to(device)\n",
    "# torch_x1= torch.from_numpy(H).double().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_decay:0.1, gcn_layer_num:1,k_size:5,learning_rate:0.01,auc:0.9308022756298617,acc:0.8829516539440203\n",
      "weight_decay:0.1, gcn_layer_num:1,k_size:50,learning_rate:0.01,auc:0.9260130035992105,acc:0.8778625954198473\n",
      "weight_decay:0.1, gcn_layer_num:1,k_size:500,learning_rate:0.01,auc:0.9312957157784744,acc:0.8854961832061069\n",
      "weight_decay:0.1, gcn_layer_num:1,k_size:5,learning_rate:0.001,auc:0.9252293045396494,acc:0.8727735368956743\n",
      "weight_decay:0.1, gcn_layer_num:1,k_size:50,learning_rate:0.001,auc:0.9258678741437363,acc:0.8651399491094147\n",
      "weight_decay:0.1, gcn_layer_num:1,k_size:500,learning_rate:0.001,auc:0.9262161848368744,acc:0.8676844783715013\n",
      "weight_decay:0.1, gcn_layer_num:2,k_size:5,learning_rate:0.01,auc:0.9278996865203761,acc:0.8778625954198473\n",
      "weight_decay:0.1, gcn_layer_num:2,k_size:50,learning_rate:0.01,auc:0.920440032508998,acc:0.8702290076335878\n",
      "weight_decay:0.1, gcn_layer_num:2,k_size:500,learning_rate:0.01,auc:0.924068268895855,acc:0.8625954198473282\n",
      "weight_decay:0.1, gcn_layer_num:2,k_size:5,learning_rate:0.001,auc:0.9248229420643214,acc:0.8651399491094147\n",
      "weight_decay:0.1, gcn_layer_num:2,k_size:50,learning_rate:0.001,auc:0.9286833855799372,acc:0.8676844783715013\n",
      "weight_decay:0.1, gcn_layer_num:2,k_size:500,learning_rate:0.001,auc:0.9241263206780448,acc:0.8625954198473282\n",
      "weight_decay:0.1, gcn_layer_num:3,k_size:5,learning_rate:0.01,auc:0.9274933240450481,acc:0.8778625954198473\n",
      "weight_decay:0.1, gcn_layer_num:3,k_size:50,learning_rate:0.01,auc:0.9268257285498664,acc:0.8727735368956743\n",
      "weight_decay:0.1, gcn_layer_num:3,k_size:500,learning_rate:0.01,auc:0.9246197608266573,acc:0.8651399491094147\n",
      "weight_decay:0.1, gcn_layer_num:3,k_size:5,learning_rate:0.001,auc:0.9279867641936608,acc:0.8702290076335878\n",
      "weight_decay:0.1, gcn_layer_num:3,k_size:50,learning_rate:0.001,auc:0.9255195634505979,acc:0.8625954198473282\n",
      "weight_decay:0.1, gcn_layer_num:3,k_size:500,learning_rate:0.001,auc:0.924706838499942,acc:0.8651399491094147\n",
      "weight_decay:0.01, gcn_layer_num:1,k_size:5,learning_rate:0.01,auc:0.9263613142923488,acc:0.8753180661577609\n",
      "weight_decay:0.01, gcn_layer_num:1,k_size:50,learning_rate:0.01,auc:0.9284511784511784,acc:0.8727735368956743\n",
      "weight_decay:0.01, gcn_layer_num:1,k_size:500,learning_rate:0.01,auc:0.930366887263439,acc:0.8727735368956743\n",
      "weight_decay:0.01, gcn_layer_num:1,k_size:5,learning_rate:0.001,auc:0.928335074886799,acc:0.8778625954198473\n",
      "weight_decay:0.01, gcn_layer_num:1,k_size:50,learning_rate:0.001,auc:0.92816091954023,acc:0.8676844783715013\n",
      "weight_decay:0.01, gcn_layer_num:1,k_size:500,learning_rate:0.001,auc:0.92308138859863,acc:0.8727735368956743\n",
      "weight_decay:0.01, gcn_layer_num:2,k_size:5,learning_rate:0.01,auc:0.9257807964704516,acc:0.8676844783715013\n",
      "weight_decay:0.01, gcn_layer_num:2,k_size:50,learning_rate:0.01,auc:0.9301346801346801,acc:0.8702290076335878\n",
      "weight_decay:0.01, gcn_layer_num:2,k_size:500,learning_rate:0.01,auc:0.9276384535005224,acc:0.8778625954198473\n",
      "weight_decay:0.01, gcn_layer_num:2,k_size:5,learning_rate:0.001,auc:0.9262742366190642,acc:0.8702290076335878\n",
      "weight_decay:0.01, gcn_layer_num:2,k_size:50,learning_rate:0.001,auc:0.9267676767676768,acc:0.8702290076335878\n",
      "weight_decay:0.01, gcn_layer_num:2,k_size:500,learning_rate:0.001,auc:0.9314988970161384,acc:0.8702290076335878\n",
      "weight_decay:0.01, gcn_layer_num:3,k_size:5,learning_rate:0.01,auc:0.9304829908278185,acc:0.8702290076335878\n",
      "weight_decay:0.01, gcn_layer_num:3,k_size:50,learning_rate:0.01,auc:0.9283641007778939,acc:0.8727735368956743\n",
      "weight_decay:0.01, gcn_layer_num:3,k_size:500,learning_rate:0.01,auc:0.9261581330546848,acc:0.8753180661577609\n",
      "weight_decay:0.01, gcn_layer_num:3,k_size:5,learning_rate:0.001,auc:0.9204980842911877,acc:0.8651399491094147\n",
      "weight_decay:0.01, gcn_layer_num:3,k_size:50,learning_rate:0.001,auc:0.9319342853825612,acc:0.8727735368956743\n",
      "weight_decay:0.01, gcn_layer_num:3,k_size:500,learning_rate:0.001,auc:0.9276965052827122,acc:0.8727735368956743\n",
      "weight_decay:0.001, gcn_layer_num:1,k_size:5,learning_rate:0.01,auc:0.9306571461743876,acc:0.8804071246819338\n",
      "weight_decay:0.001, gcn_layer_num:1,k_size:50,learning_rate:0.01,auc:0.9280448159758505,acc:0.8778625954198473\n",
      "weight_decay:0.001, gcn_layer_num:1,k_size:500,learning_rate:0.01,auc:0.9250551491930803,acc:0.8727735368956743\n",
      "weight_decay:0.001, gcn_layer_num:1,k_size:5,learning_rate:0.001,auc:0.926709624985487,acc:0.8676844783715013\n",
      "weight_decay:0.001, gcn_layer_num:1,k_size:50,learning_rate:0.001,auc:0.9280738418669453,acc:0.8651399491094147\n",
      "weight_decay:0.001, gcn_layer_num:1,k_size:500,learning_rate:0.001,auc:0.9233716475095786,acc:0.8778625954198473\n",
      "weight_decay:0.001, gcn_layer_num:2,k_size:5,learning_rate:0.01,auc:0.9297573435504469,acc:0.8702290076335878\n",
      "weight_decay:0.001, gcn_layer_num:2,k_size:50,learning_rate:0.01,auc:0.9261581330546849,acc:0.8727735368956743\n",
      "weight_decay:0.001, gcn_layer_num:2,k_size:500,learning_rate:0.01,auc:0.9269128062231511,acc:0.8702290076335878\n",
      "weight_decay:0.001, gcn_layer_num:2,k_size:5,learning_rate:0.001,auc:0.928741437362127,acc:0.8727735368956743\n",
      "weight_decay:0.001, gcn_layer_num:2,k_size:50,learning_rate:0.001,auc:0.9239811912225705,acc:0.8702290076335878\n",
      "weight_decay:0.001, gcn_layer_num:2,k_size:500,learning_rate:0.001,auc:0.9227330779054916,acc:0.8651399491094147\n",
      "weight_decay:0.001, gcn_layer_num:3,k_size:5,learning_rate:0.01,auc:0.9249390456287009,acc:0.8753180661577609\n",
      "weight_decay:0.001, gcn_layer_num:3,k_size:50,learning_rate:0.01,auc:0.9353012887495646,acc:0.8854961832061069\n",
      "weight_decay:0.001, gcn_layer_num:3,k_size:500,learning_rate:0.01,auc:0.9282770231046094,acc:0.8753180661577609\n",
      "weight_decay:0.001, gcn_layer_num:3,k_size:5,learning_rate:0.001,auc:0.9226750261233021,acc:0.8702290076335878\n",
      "weight_decay:0.001, gcn_layer_num:3,k_size:50,learning_rate:0.001,auc:0.9256066411238825,acc:0.8702290076335878\n",
      "weight_decay:0.001, gcn_layer_num:3,k_size:500,learning_rate:0.001,auc:0.9266515732032974,acc:0.8753180661577609\n",
      "weight_decay:0, gcn_layer_num:1,k_size:5,learning_rate:0.01,auc:0.9259259259259259,acc:0.8702290076335878\n",
      "weight_decay:0, gcn_layer_num:1,k_size:50,learning_rate:0.01,auc:0.9263613142923488,acc:0.8676844783715013\n",
      "weight_decay:0, gcn_layer_num:1,k_size:500,learning_rate:0.01,auc:0.9236038546383374,acc:0.8804071246819338\n",
      "weight_decay:0, gcn_layer_num:1,k_size:5,learning_rate:0.001,auc:0.9234296992917683,acc:0.8625954198473282\n",
      "weight_decay:0, gcn_layer_num:1,k_size:50,learning_rate:0.001,auc:0.9256937187971671,acc:0.8651399491094147\n",
      "weight_decay:0, gcn_layer_num:1,k_size:500,learning_rate:0.001,auc:0.9255195634505979,acc:0.8676844783715013\n",
      "weight_decay:0, gcn_layer_num:2,k_size:5,learning_rate:0.01,auc:0.9281318936491351,acc:0.8804071246819338\n",
      "weight_decay:0, gcn_layer_num:2,k_size:50,learning_rate:0.01,auc:0.9223557413212585,acc:0.8702290076335878\n",
      "weight_decay:0, gcn_layer_num:2,k_size:500,learning_rate:0.01,auc:0.9279577383025659,acc:0.8676844783715013\n",
      "weight_decay:0, gcn_layer_num:2,k_size:5,learning_rate:0.001,auc:0.9225589225589226,acc:0.8651399491094147\n",
      "weight_decay:0, gcn_layer_num:2,k_size:50,learning_rate:0.001,auc:0.9210786021130848,acc:0.8625954198473282\n",
      "weight_decay:0, gcn_layer_num:2,k_size:500,learning_rate:0.001,auc:0.9278706606292814,acc:0.8625954198473282\n",
      "weight_decay:0, gcn_layer_num:3,k_size:5,learning_rate:0.01,auc:0.9259839777081156,acc:0.8829516539440203\n",
      "weight_decay:0, gcn_layer_num:3,k_size:50,learning_rate:0.01,auc:0.9280448159758504,acc:0.8753180661577609\n",
      "weight_decay:0, gcn_layer_num:3,k_size:500,learning_rate:0.01,auc:0.9251712527574597,acc:0.8702290076335878\n",
      "weight_decay:0, gcn_layer_num:3,k_size:5,learning_rate:0.001,auc:0.9269128062231511,acc:0.8625954198473282\n",
      "weight_decay:0, gcn_layer_num:3,k_size:50,learning_rate:0.001,auc:0.9219203529548357,acc:0.8625954198473282\n",
      "weight_decay:0, gcn_layer_num:3,k_size:500,learning_rate:0.001,auc:0.9224137931034483,acc:0.8600508905852418\n"
     ]
    }
   ],
   "source": [
    "record ={}\n",
    "for weight_decay in [0.1,0.01,0.001,0]:\n",
    "    record [weight_decay]={}\n",
    "    for gcn_layer_num in [1,2,3]:\n",
    "        record[weight_decay][gcn_layer_num]={}\n",
    "        for learning_rate in [0.01,0.001]:\n",
    "            record[weight_decay][gcn_layer_num][learning_rate]={}\n",
    "            for k_size in [5,50,500]:# [50,100,150,200,250,300,350,400,450,500]:\n",
    "                record[weight_decay][gcn_layer_num][learning_rate][k_size]=[0,0]\n",
    "                \n",
    "                # model\n",
    "                model = gcn(40,32,k_size,gcn_layer_num,P,device)\n",
    "                # model = Linear1(48,512,500,P,device)\n",
    "                model.double().to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.99),weight_decay=weight_decay)\n",
    "                criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=40, verbose=False,\n",
    "                                                                       min_lr=1e-6)\n",
    "                # training \n",
    "                model.train()\n",
    "                for epoch in (range(max_epoch)):\n",
    "                    optimizer.zero_grad()\n",
    "                    start=time.time()\n",
    "                    out = model(torch_x1,torch_x2)\n",
    "                    loss = criterion(out, train_val_labels)\n",
    "                    train_loss= loss[train_flag==1].mean()\n",
    "                    val_loss= loss[train_flag==0].mean()\n",
    "                    train_loss.backward()\n",
    "                    #     print(train_loss)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    predict = out[:,1].detach().to('cpu').numpy()\n",
    "                    acc_predict= out.argmax(dim=1).detach().to('cpu').numpy()\n",
    "                    np_train_labels = train_val_labels.detach().to('cpu').numpy()\n",
    "\n",
    "                    auc_train= roc_auc_score(y_true=np_train_labels[train_flag.numpy()==1],y_score=predict[train_flag.numpy()==1])\n",
    "                    auc_val =roc_auc_score(y_true=np_train_labels[train_flag.numpy()==0],y_score=predict[train_flag.numpy()==0] )\n",
    "\n",
    "                    acc_train = accuracy_score(np_train_labels[train_flag.numpy()==1],acc_predict[train_flag.numpy()==1])\n",
    "                    acc_val = accuracy_score(np_train_labels[train_flag.numpy()==0],acc_predict[train_flag.numpy()==0])\n",
    "\n",
    "                    scheduler.step(auc_val)\n",
    "                    #                     print('train: [%4d/ %5d], train loss: %.6f, val loss : %.6f, train auc: %.6f, val auc: %.6f, train acc: %.6f, val acc: %.6f,  time: %f s' %\n",
    "                    #                               (epoch + 1, max_epoch, train_loss.item(), val_loss.item(), auc_train,auc_val,acc_train,acc_val, time.time() - start))\n",
    "                    record[weight_decay][gcn_layer_num][learning_rate][k_size]=[max(record[weight_decay][gcn_layer_num][learning_rate][k_size][0],auc_val),\n",
    "                                                                                max(record[weight_decay][gcn_layer_num][learning_rate][k_size][1],acc_val)]\n",
    "                    \n",
    "                    json.dump(record,open('parameter_select.json','w'))\n",
    "                print ('weight_decay:{}, gcn_layer_num:{},k_size:{},learning_rate:{},auc:{},acc:{}'.format(weight_decay,gcn_layer_num,k_size,learning_rate,\\\n",
    "                                                                                                           record[weight_decay][gcn_layer_num][learning_rate][k_size][0],\\\n",
    "                                                                                                          record[weight_decay][gcn_layer_num][learning_rate][k_size][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_decay:0.1, gcn_layer_num:0,k_size:5,learning_rate:0.01,auc:0.9287704632532219,acc:0.8651399491094147\n",
      "weight_decay:0.1, gcn_layer_num:0,k_size:50,learning_rate:0.01,auc:0.9261871589457796,acc:0.8676844783715013\n",
      "weight_decay:0.1, gcn_layer_num:0,k_size:500,learning_rate:0.01,auc:0.9255776152327877,acc:0.8753180661577609\n",
      "weight_decay:0.1, gcn_layer_num:0,k_size:5,learning_rate:0.001,auc:0.9258098223615465,acc:0.8651399491094147\n",
      "weight_decay:0.1, gcn_layer_num:0,k_size:50,learning_rate:0.001,auc:0.9232845698362939,acc:0.8727735368956743\n",
      "weight_decay:0.1, gcn_layer_num:0,k_size:500,learning_rate:0.001,auc:0.9269708580053408,acc:0.8727735368956743\n",
      "weight_decay:0.1, gcn_layer_num:1,k_size:5,learning_rate:0.01,auc:0.9301056542435853,acc:0.8753180661577609\n",
      "weight_decay:0.1, gcn_layer_num:1,k_size:50,learning_rate:0.01,auc:0.9240102171136654,acc:0.8727735368956743\n",
      "weight_decay:0.1, gcn_layer_num:1,k_size:500,learning_rate:0.01,auc:0.9328921397886916,acc:0.8676844783715013\n",
      "weight_decay:0.1, gcn_layer_num:1,k_size:5,learning_rate:0.001,auc:0.9285963079066527,acc:0.8702290076335878\n",
      "weight_decay:0.1, gcn_layer_num:1,k_size:50,learning_rate:0.001,auc:0.9255776152327877,acc:0.8676844783715013\n",
      "weight_decay:0.1, gcn_layer_num:1,k_size:500,learning_rate:0.001,auc:0.9303959131545338,acc:0.8651399491094147\n",
      "weight_decay:0.1, gcn_layer_num:2,k_size:5,learning_rate:0.01,auc:0.9267096249854871,acc:0.8676844783715013\n",
      "weight_decay:0.1, gcn_layer_num:2,k_size:50,learning_rate:0.01,auc:0.9244456054800883,acc:0.8651399491094147\n",
      "weight_decay:0.1, gcn_layer_num:2,k_size:500,learning_rate:0.01,auc:0.929554162312783,acc:0.8804071246819338\n",
      "weight_decay:0.1, gcn_layer_num:2,k_size:5,learning_rate:0.001,auc:0.9263322884012539,acc:0.8651399491094147\n",
      "weight_decay:0.1, gcn_layer_num:2,k_size:50,learning_rate:0.001,auc:0.9249970974108905,acc:0.8625954198473282\n",
      "weight_decay:0.1, gcn_layer_num:2,k_size:500,learning_rate:0.001,auc:0.9245617090444677,acc:0.8651399491094147\n",
      "weight_decay:0.1, gcn_layer_num:3,k_size:5,learning_rate:0.01,auc:0.9272611169162893,acc:0.8753180661577609\n",
      "weight_decay:0.1, gcn_layer_num:3,k_size:50,learning_rate:0.01,auc:0.928944618599791,acc:0.8676844783715013\n",
      "weight_decay:0.1, gcn_layer_num:3,k_size:500,learning_rate:0.01,auc:0.9276965052827122,acc:0.8753180661577609\n",
      "weight_decay:0.1, gcn_layer_num:3,k_size:5,learning_rate:0.001,auc:0.9241553465691397,acc:0.8676844783715013\n",
      "weight_decay:0.1, gcn_layer_num:3,k_size:50,learning_rate:0.001,auc:0.9254905375595031,acc:0.8702290076335878\n",
      "weight_decay:0.1, gcn_layer_num:3,k_size:500,learning_rate:0.001,auc:0.926303262510159,acc:0.8651399491094147\n",
      "weight_decay:0.01, gcn_layer_num:0,k_size:5,learning_rate:0.01,auc:0.9278706606292814,acc:0.8778625954198473\n",
      "weight_decay:0.01, gcn_layer_num:0,k_size:50,learning_rate:0.01,auc:0.9302507836990596,acc:0.8778625954198473\n",
      "weight_decay:0.01, gcn_layer_num:0,k_size:500,learning_rate:0.01,auc:0.92612910716359,acc:0.8727735368956743\n",
      "weight_decay:0.01, gcn_layer_num:0,k_size:5,learning_rate:0.001,auc:0.92612910716359,acc:0.8702290076335878\n",
      "weight_decay:0.01, gcn_layer_num:0,k_size:50,learning_rate:0.001,auc:0.9271740392430048,acc:0.8727735368956743\n",
      "weight_decay:0.01, gcn_layer_num:0,k_size:500,learning_rate:0.001,auc:0.9262742366190642,acc:0.8676844783715013\n",
      "weight_decay:0.01, gcn_layer_num:1,k_size:5,learning_rate:0.01,auc:0.9283060489957042,acc:0.8753180661577609\n",
      "weight_decay:0.01, gcn_layer_num:1,k_size:50,learning_rate:0.01,auc:0.9302507836990596,acc:0.8778625954198473\n",
      "weight_decay:0.01, gcn_layer_num:1,k_size:500,learning_rate:0.01,auc:0.930163706025775,acc:0.8753180661577609\n",
      "weight_decay:0.01, gcn_layer_num:1,k_size:5,learning_rate:0.001,auc:0.9273191686984791,acc:0.8651399491094147\n",
      "weight_decay:0.01, gcn_layer_num:1,k_size:50,learning_rate:0.001,auc:0.9229943109253453,acc:0.8727735368956743\n",
      "weight_decay:0.01, gcn_layer_num:1,k_size:500,learning_rate:0.001,auc:0.927928712411471,acc:0.8753180661577609\n",
      "weight_decay:0.01, gcn_layer_num:2,k_size:5,learning_rate:0.01,auc:0.9290026703819808,acc:0.8778625954198473\n",
      "weight_decay:0.01, gcn_layer_num:2,k_size:50,learning_rate:0.01,auc:0.9284221525600836,acc:0.8778625954198473\n",
      "weight_decay:0.01, gcn_layer_num:2,k_size:500,learning_rate:0.01,auc:0.9244165795889934,acc:0.8702290076335878\n",
      "weight_decay:0.01, gcn_layer_num:2,k_size:5,learning_rate:0.001,auc:0.9278416347381865,acc:0.8651399491094147\n",
      "weight_decay:0.01, gcn_layer_num:2,k_size:50,learning_rate:0.001,auc:0.92612910716359,acc:0.8727735368956743\n",
      "weight_decay:0.01, gcn_layer_num:2,k_size:500,learning_rate:0.001,auc:0.9280448159758505,acc:0.8727735368956743\n",
      "weight_decay:0.01, gcn_layer_num:3,k_size:5,learning_rate:0.01,auc:0.9286543596888425,acc:0.8753180661577609\n",
      "weight_decay:0.01, gcn_layer_num:3,k_size:50,learning_rate:0.01,auc:0.9283060489957041,acc:0.8829516539440203\n",
      "weight_decay:0.01, gcn_layer_num:3,k_size:500,learning_rate:0.01,auc:0.9288285150354116,acc:0.8778625954198473\n",
      "weight_decay:0.01, gcn_layer_num:3,k_size:5,learning_rate:0.001,auc:0.9215430163706025,acc:0.8702290076335878\n",
      "weight_decay:0.01, gcn_layer_num:3,k_size:50,learning_rate:0.001,auc:0.9274933240450481,acc:0.8676844783715013\n",
      "weight_decay:0.01, gcn_layer_num:3,k_size:500,learning_rate:0.001,auc:0.9258388482526414,acc:0.8676844783715013\n",
      "weight_decay:0.001, gcn_layer_num:0,k_size:5,learning_rate:0.01,auc:0.9294090328573087,acc:0.8753180661577609\n",
      "weight_decay:0.001, gcn_layer_num:0,k_size:50,learning_rate:0.01,auc:0.9279867641936608,acc:0.8778625954198473\n",
      "weight_decay:0.001, gcn_layer_num:0,k_size:500,learning_rate:0.01,auc:0.9283060489957041,acc:0.8753180661577609\n",
      "weight_decay:0.001, gcn_layer_num:1,k_size:5,learning_rate:0.01,auc:0.9306571461743876,acc:0.8753180661577609\n",
      "weight_decay:0.001, gcn_layer_num:1,k_size:50,learning_rate:0.01,auc:0.9276674793916173,acc:0.8778625954198473\n",
      "weight_decay:0.001, gcn_layer_num:1,k_size:500,learning_rate:0.01,auc:0.9288285150354115,acc:0.8753180661577609\n",
      "weight_decay:0.001, gcn_layer_num:1,k_size:5,learning_rate:0.001,auc:0.9264193660745385,acc:0.8676844783715013\n",
      "weight_decay:0.001, gcn_layer_num:1,k_size:50,learning_rate:0.001,auc:0.9269708580053407,acc:0.8702290076335878\n",
      "weight_decay:0.001, gcn_layer_num:1,k_size:500,learning_rate:0.001,auc:0.9238360617670962,acc:0.8727735368956743\n",
      "weight_decay:0.001, gcn_layer_num:2,k_size:5,learning_rate:0.01,auc:0.9298153953326367,acc:0.8753180661577609\n",
      "weight_decay:0.001, gcn_layer_num:2,k_size:50,learning_rate:0.01,auc:0.9310925345408104,acc:0.8829516539440203\n",
      "weight_decay:0.001, gcn_layer_num:2,k_size:500,learning_rate:0.01,auc:0.9305990943921978,acc:0.8778625954198473\n",
      "weight_decay:0.001, gcn_layer_num:2,k_size:5,learning_rate:0.001,auc:0.9267967026587717,acc:0.8753180661577609\n",
      "weight_decay:0.001, gcn_layer_num:2,k_size:50,learning_rate:0.001,auc:0.9276384535005225,acc:0.8727735368956743\n",
      "weight_decay:0.001, gcn_layer_num:2,k_size:500,learning_rate:0.001,auc:0.9245907349355625,acc:0.8676844783715013\n",
      "weight_decay:0.001, gcn_layer_num:3,k_size:5,learning_rate:0.01,auc:0.9316440264716127,acc:0.8778625954198473\n",
      "weight_decay:0.001, gcn_layer_num:3,k_size:50,learning_rate:0.01,auc:0.9313537675606641,acc:0.8804071246819338\n",
      "weight_decay:0.001, gcn_layer_num:3,k_size:500,learning_rate:0.01,auc:0.9259259259259259,acc:0.8727735368956743\n",
      "weight_decay:0.001, gcn_layer_num:3,k_size:5,learning_rate:0.001,auc:0.9262161848368745,acc:0.8651399491094147\n",
      "weight_decay:0.001, gcn_layer_num:3,k_size:50,learning_rate:0.001,auc:0.9229943109253455,acc:0.8676844783715013\n",
      "weight_decay:0.001, gcn_layer_num:3,k_size:500,learning_rate:0.001,auc:0.9255195634505979,acc:0.8727735368956743\n",
      "weight_decay:0, gcn_layer_num:0,k_size:5,learning_rate:0.01,auc:0.9248809938465111,acc:0.8727735368956743\n",
      "weight_decay:0, gcn_layer_num:0,k_size:50,learning_rate:0.01,auc:0.9233135957273888,acc:0.8753180661577609\n",
      "weight_decay:0, gcn_layer_num:0,k_size:500,learning_rate:0.01,auc:0.926709624985487,acc:0.8804071246819338\n",
      "weight_decay:0, gcn_layer_num:0,k_size:5,learning_rate:0.001,auc:0.9265064437478231,acc:0.8676844783715013\n",
      "weight_decay:0, gcn_layer_num:0,k_size:50,learning_rate:0.001,auc:0.9276674793916173,acc:0.8651399491094147\n",
      "weight_decay:0, gcn_layer_num:0,k_size:500,learning_rate:0.001,auc:0.9245326831533729,acc:0.8625954198473282\n",
      "weight_decay:0, gcn_layer_num:1,k_size:5,learning_rate:0.01,auc:0.9247939161732266,acc:0.8753180661577609\n",
      "weight_decay:0, gcn_layer_num:1,k_size:50,learning_rate:0.01,auc:0.9312957157784745,acc:0.8804071246819338\n",
      "weight_decay:0, gcn_layer_num:1,k_size:500,learning_rate:0.01,auc:0.9263613142923488,acc:0.8727735368956743\n",
      "weight_decay:0, gcn_layer_num:1,k_size:5,learning_rate:0.001,auc:0.9303088354812492,acc:0.8702290076335878\n",
      "weight_decay:0, gcn_layer_num:1,k_size:50,learning_rate:0.001,auc:0.9193370486473935,acc:0.8676844783715013\n",
      "weight_decay:0, gcn_layer_num:1,k_size:500,learning_rate:0.001,auc:0.9275513758272379,acc:0.8727735368956743\n",
      "weight_decay:0, gcn_layer_num:2,k_size:5,learning_rate:0.01,auc:0.928944618599791,acc:0.8778625954198473\n",
      "weight_decay:0, gcn_layer_num:2,k_size:50,learning_rate:0.01,auc:0.9275804017183327,acc:0.8778625954198473\n",
      "weight_decay:0, gcn_layer_num:2,k_size:500,learning_rate:0.01,auc:0.9222106118657843,acc:0.8702290076335878\n",
      "weight_decay:0, gcn_layer_num:2,k_size:5,learning_rate:0.001,auc:0.9246778126088471,acc:0.8625954198473282\n",
      "weight_decay:0, gcn_layer_num:2,k_size:50,learning_rate:0.001,auc:0.9227621037965865,acc:0.8702290076335878\n",
      "weight_decay:0, gcn_layer_num:2,k_size:500,learning_rate:0.001,auc:0.9221525600835945,acc:0.8625954198473282\n",
      "weight_decay:0, gcn_layer_num:3,k_size:5,learning_rate:0.01,auc:0.9241263206780448,acc:0.8778625954198473\n",
      "weight_decay:0, gcn_layer_num:3,k_size:50,learning_rate:0.01,auc:0.9262452107279694,acc:0.8778625954198473\n",
      "weight_decay:0, gcn_layer_num:3,k_size:500,learning_rate:0.01,auc:0.9271450133519098,acc:0.8702290076335878\n",
      "weight_decay:0, gcn_layer_num:3,k_size:5,learning_rate:0.001,auc:0.9244456054800883,acc:0.8727735368956743\n",
      "weight_decay:0, gcn_layer_num:3,k_size:50,learning_rate:0.001,auc:0.9274352722628585,acc:0.8702290076335878\n",
      "weight_decay:0, gcn_layer_num:3,k_size:500,learning_rate:0.001,auc:0.9252583304307442,acc:0.8600508905852418\n"
     ]
    }
   ],
   "source": [
    "record ={}\n",
    "for weight_decay in [0.1,0.01,0.001,0]:\n",
    "    record [weight_decay]={}\n",
    "    for gcn_layer_num in [0,1,2,3]:\n",
    "        record[weight_decay][gcn_layer_num]={}\n",
    "        for learning_rate in [0.01,0.001]:\n",
    "            record[weight_decay][gcn_layer_num][learning_rate]={}\n",
    "            for k_size in [5,50,500]:# [50,100,150,200,250,300,350,400,450,500]:\n",
    "                record[weight_decay][gcn_layer_num][learning_rate][k_size]=[0,0]\n",
    "                \n",
    "                # model\n",
    "                model = gcn(40,32,k_size,gcn_layer_num,P,device)\n",
    "                # model = Linear1(48,512,500,P,device)\n",
    "                model.double().to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.99),weight_decay=weight_decay)\n",
    "                criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=40, verbose=False,\n",
    "                                                                       min_lr=1e-6)\n",
    "                # training \n",
    "                model.train()\n",
    "                for epoch in (range(max_epoch)):\n",
    "                    optimizer.zero_grad()\n",
    "                    start=time.time()\n",
    "                    out = model(torch_x1,torch_x2)\n",
    "                    loss = criterion(out, train_val_labels)\n",
    "                    train_loss= loss[train_flag==1].mean()\n",
    "                    val_loss= loss[train_flag==0].mean()\n",
    "                    train_loss.backward()\n",
    "                    #     print(train_loss)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    predict = out[:,1].detach().to('cpu').numpy()\n",
    "                    acc_predict= out.argmax(dim=1).detach().to('cpu').numpy()\n",
    "                    np_train_labels = train_val_labels.detach().to('cpu').numpy()\n",
    "\n",
    "                    auc_train= roc_auc_score(y_true=np_train_labels[train_flag.numpy()==1],y_score=predict[train_flag.numpy()==1])\n",
    "                    auc_val =roc_auc_score(y_true=np_train_labels[train_flag.numpy()==0],y_score=predict[train_flag.numpy()==0] )\n",
    "\n",
    "                    acc_train = accuracy_score(np_train_labels[train_flag.numpy()==1],acc_predict[train_flag.numpy()==1])\n",
    "                    acc_val = accuracy_score(np_train_labels[train_flag.numpy()==0],acc_predict[train_flag.numpy()==0])\n",
    "\n",
    "                    scheduler.step(auc_val)\n",
    "                    #                     print('train: [%4d/ %5d], train loss: %.6f, val loss : %.6f, train auc: %.6f, val auc: %.6f, train acc: %.6f, val acc: %.6f,  time: %f s' %\n",
    "                    #                               (epoch + 1, max_epoch, train_loss.item(), val_loss.item(), auc_train,auc_val,acc_train,acc_val, time.time() - start))\n",
    "                    record[weight_decay][gcn_layer_num][learning_rate][k_size]=[max(record[weight_decay][gcn_layer_num][learning_rate][k_size][0],auc_val),\n",
    "                                                                                max(record[weight_decay][gcn_layer_num][learning_rate][k_size][1],acc_val)]\n",
    "                    \n",
    "                    json.dump(record,open('parameter_select.json','w'))\n",
    "                print ('weight_decay:{}, gcn_layer_num:{},k_size:{},learning_rate:{},auc:{},acc:{}'.format(weight_decay,gcn_layer_num,k_size,learning_rate,\\\n",
    "                                                                                                           record[weight_decay][gcn_layer_num][learning_rate][k_size][0],\\\n",
    "                                                                                                          record[weight_decay][gcn_layer_num][learning_rate][k_size][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0.1, 0.01, 0.001]\n",
      "auc:0.9316150005805178,acc:0.8778625954198473\n",
      "[1, 0.2, 0.01, 0.001]\n",
      "auc:0.9316150005805178,acc:0.8778625954198473\n",
      "[1, 0.3, 0.01, 0.001]\n",
      "auc:0.9316150005805178,acc:0.8778625954198473\n",
      "[1, 0.4, 0.01, 0.001]\n",
      "auc:0.9316150005805178,acc:0.8778625954198473\n",
      "[1, 0.5, 0.01, 0.001]\n",
      "auc:0.9316150005805178,acc:0.8778625954198473\n",
      "[1, 0.6, 0.01, 0.001]\n",
      "auc:0.9316150005805178,acc:0.8804071246819338\n",
      "[1, 0.7, 0.01, 0.001]\n",
      "auc:0.9316150005805178,acc:0.8804071246819338\n",
      "[1, 0.8, 0.01, 0.001]\n",
      "auc:0.9316150005805178,acc:0.8804071246819338\n",
      "[1, 0.9, 0.01, 0.001]\n",
      "auc:0.9316150005805178,acc:0.8804071246819338\n",
      "[1, 1, 0.01, 0.001]\n",
      "auc:0.9317020782538024,acc:0.8804071246819338\n"
     ]
    }
   ],
   "source": [
    "record = [0,0]\n",
    "weight_decay =0.01\n",
    "gcn_layer_num =1\n",
    "learning_rate =0.01\n",
    "k_size = 50# [50,100,150,200,250,300,350,400,450,500]:\n",
    "alphas=[[1,0.1,0.01,0.001],\n",
    "        [1,0.2,0.01,0.001],\n",
    "        [1,0.3,0.01,0.001],\n",
    "        [1,0.4,0.01,0.001],\n",
    "        [1,0.5,0.01,0.001],\n",
    "        [1,0.6,0.01,0.001],\n",
    "        [1,0.7,0.01,0.001],\n",
    "        [1,0.8,0.01,0.001],\n",
    "        [1,0.9,0.01,0.001],\n",
    "        [1,1,0.01,0.001]]\n",
    "for alpha in alphas :\n",
    "    # model\n",
    "    model = gcn(40,32,k_size,gcn_layer_num,P,device,alpha)\n",
    "    # model = Linear1(48,512,500,P,device)\n",
    "    model.double().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.99),weight_decay=weight_decay)\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=40, verbose=False,\n",
    "                                                           min_lr=1e-6)\n",
    "    # training \n",
    "    model.train()\n",
    "    for epoch in (range(max_epoch)):\n",
    "        optimizer.zero_grad()\n",
    "        start=time.time()\n",
    "        out = model(torch_x1,torch_x2)\n",
    "        loss = criterion(out, train_val_labels)\n",
    "        train_loss= loss[train_flag==1].mean()\n",
    "        val_loss= loss[train_flag==0].mean()\n",
    "        train_loss.backward()\n",
    "        #     print(train_loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        predict = out[:,1].detach().to('cpu').numpy()\n",
    "        acc_predict= out.argmax(dim=1).detach().to('cpu').numpy()\n",
    "        np_train_labels = train_val_labels.detach().to('cpu').numpy()\n",
    "\n",
    "        auc_train= roc_auc_score(y_true=np_train_labels[train_flag.numpy()==1],y_score=predict[train_flag.numpy()==1])\n",
    "        auc_val =roc_auc_score(y_true=np_train_labels[train_flag.numpy()==0],y_score=predict[train_flag.numpy()==0] )\n",
    "\n",
    "        acc_train = accuracy_score(np_train_labels[train_flag.numpy()==1],acc_predict[train_flag.numpy()==1])\n",
    "        acc_val = accuracy_score(np_train_labels[train_flag.numpy()==0],acc_predict[train_flag.numpy()==0])\n",
    "\n",
    "        scheduler.step(auc_val)\n",
    "        #                     print('train: [%4d/ %5d], train loss: %.6f, val loss : %.6f, train auc: %.6f, val auc: %.6f, train acc: %.6f, val acc: %.6f,  time: %f s' %\n",
    "        #                               (epoch + 1, max_epoch, train_loss.item(), val_loss.item(), auc_train,auc_val,acc_train,acc_val, time.time() - start))\n",
    "#         record[weight_decay][gcn_layer_num][learning_rate][k_size]=[max(record[weight_decay][gcn_layer_num][learning_rate][k_size][0],auc_val),\n",
    "#                                                                     max(record[weight_decay][gcn_layer_num][learning_rate][k_size][1],acc_val)]\n",
    "        record =[max(record[0],auc_val),max(record[1],acc_val)]\n",
    "\n",
    "        json.dump(record,open('parameter_select.json','w'))\n",
    "    print(alpha)\n",
    "    print('auc:{},acc:{}'.format(record[0],record[1]))\n",
    "#     print ('weight_decay:{}, gcn_layer_num:{},k_size:{},learning_rate:{},auc:{},acc:{}'.format(weight_decay,gcn_layer_num,k_size,learning_rate,\\\n",
    "#                                                                                                record[weight_decay][gcn_layer_num][learning_rate][k_size][0],\\\n",
    "#                                                                                               record[weight_decay][gcn_layer_num][learning_rate][k_size][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0.1, 0.1, 0.001]\n",
      "auc:0.9292348775107396,acc:0.8778625954198473\n",
      "[1, 0.2, 0.2, 0.001]\n",
      "auc:0.9306571461743874,acc:0.8778625954198473\n",
      "[1, 0.3, 0.3, 0.001]\n",
      "auc:0.9306571461743874,acc:0.8778625954198473\n",
      "[1, 0.4, 0.4, 0.001]\n",
      "auc:0.9306571461743874,acc:0.8778625954198473\n",
      "[1, 0.5, 0.5, 0.001]\n",
      "auc:0.9306571461743874,acc:0.8778625954198473\n",
      "[1, 0.6, 0.6, 0.001]\n",
      "auc:0.9306571461743874,acc:0.8778625954198473\n",
      "[1, 0.7, 0.7, 0.001]\n",
      "auc:0.9306571461743874,acc:0.8778625954198473\n",
      "[1, 0.8, 0.8, 0.001]\n",
      "auc:0.9306571461743874,acc:0.8778625954198473\n",
      "[1, 0.9, 0.9, 0.001]\n",
      "auc:0.9306571461743874,acc:0.8778625954198473\n",
      "[1, 1, 1, 0.001]\n",
      "auc:0.9306571461743874,acc:0.8778625954198473\n"
     ]
    }
   ],
   "source": [
    "record = [0,0]\n",
    "weight_decay = 0.001\n",
    "gcn_layer_num =2\n",
    "learning_rate =0.01\n",
    "k_size = 50# [50,100,150,200,250,300,350,400,450,500]:\n",
    "alphas=[[1,0.1,0.1,0.001],\n",
    "        [1,0.2,0.2,0.001],\n",
    "        [1,0.3,0.3,0.001],\n",
    "        [1,0.4,0.4,0.001],\n",
    "        [1,0.5,0.5,0.001],\n",
    "        [1,0.6,0.6,0.001],\n",
    "        [1,0.7,0.7,0.001],\n",
    "        [1,0.8,0.8,0.001],\n",
    "        [1,0.9,0.9,0.001],\n",
    "        [1,1,1,0.001]]\n",
    "for alpha in alphas :\n",
    "    # model\n",
    "    model = gcn(40,32,k_size,gcn_layer_num,P,device,alpha)\n",
    "    # model = Linear1(48,512,500,P,device)\n",
    "    model.double().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.99),weight_decay=weight_decay)\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=40, verbose=False,\n",
    "                                                           min_lr=1e-6)\n",
    "    # training \n",
    "    model.train()\n",
    "    for epoch in (range(max_epoch)):\n",
    "        optimizer.zero_grad()\n",
    "        start=time.time()\n",
    "        out = model(torch_x1,torch_x2)\n",
    "        loss = criterion(out, train_val_labels)\n",
    "        train_loss= loss[train_flag==1].mean()\n",
    "        val_loss= loss[train_flag==0].mean()\n",
    "        train_loss.backward()\n",
    "        #     print(train_loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        predict = out[:,1].detach().to('cpu').numpy()\n",
    "        acc_predict= out.argmax(dim=1).detach().to('cpu').numpy()\n",
    "        np_train_labels = train_val_labels.detach().to('cpu').numpy()\n",
    "\n",
    "        auc_train= roc_auc_score(y_true=np_train_labels[train_flag.numpy()==1],y_score=predict[train_flag.numpy()==1])\n",
    "        auc_val =roc_auc_score(y_true=np_train_labels[train_flag.numpy()==0],y_score=predict[train_flag.numpy()==0] )\n",
    "\n",
    "        acc_train = accuracy_score(np_train_labels[train_flag.numpy()==1],acc_predict[train_flag.numpy()==1])\n",
    "        acc_val = accuracy_score(np_train_labels[train_flag.numpy()==0],acc_predict[train_flag.numpy()==0])\n",
    "\n",
    "        scheduler.step(auc_val)\n",
    "        #                     print('train: [%4d/ %5d], train loss: %.6f, val loss : %.6f, train auc: %.6f, val auc: %.6f, train acc: %.6f, val acc: %.6f,  time: %f s' %\n",
    "        #                               (epoch + 1, max_epoch, train_loss.item(), val_loss.item(), auc_train,auc_val,acc_train,acc_val, time.time() - start))\n",
    "#         record[weight_decay][gcn_layer_num][learning_rate][k_size]=[max(record[weight_decay][gcn_layer_num][learning_rate][k_size][0],auc_val),\n",
    "#                                                                     max(record[weight_decay][gcn_layer_num][learning_rate][k_size][1],acc_val)]\n",
    "        record =[max(record[0],auc_val),max(record[1],acc_val)]\n",
    "\n",
    "        json.dump(record,open('parameter_select.json','w'))\n",
    "    print(alpha)\n",
    "    print('auc:{},acc:{}'.format(record[0],record[1]))\n",
    "#     print ('weight_decay:{}, gcn_layer_num:{},k_size:{},learning_rate:{},auc:{},acc:{}'.format(weight_decay,gcn_layer_num,k_size,learning_rate,\\\n",
    "#                                                                                                record[weight_decay][gcn_layer_num][learning_rate][k_size][0],\\\n",
    "#                                                                                               record[weight_decay][gcn_layer_num][learning_rate][k_size][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0.1, 0.1, 0.1]\n",
      "auc:0.9330662951352607,acc:0.8753180661577609\n",
      "[1, 0.2, 0.2, 0.2]\n",
      "auc:0.9330662951352607,acc:0.8778625954198473\n",
      "[1, 0.3, 0.3, 0.3]\n",
      "auc:0.9330662951352607,acc:0.8778625954198473\n",
      "[1, 0.4, 0.4, 0.4]\n",
      "auc:0.9330662951352607,acc:0.8854961832061069\n",
      "[1, 0.5, 0.5, 0.5]\n",
      "auc:0.9330662951352607,acc:0.8854961832061069\n",
      "[1, 0.6, 0.6, 0.6]\n",
      "auc:0.9330662951352607,acc:0.8854961832061069\n",
      "[1, 0.7, 0.7, 0.7]\n",
      "auc:0.9330662951352607,acc:0.8854961832061069\n",
      "[1, 0.8, 0.8, 0.8]\n",
      "auc:0.9330662951352607,acc:0.8854961832061069\n",
      "[1, 0.9, 0.9, 0.9]\n",
      "auc:0.9330662951352607,acc:0.8854961832061069\n",
      "[1, 1, 1, 1]\n",
      "auc:0.9330662951352607,acc:0.8854961832061069\n"
     ]
    }
   ],
   "source": [
    "record = [0,0]\n",
    "weight_decay = 0.001\n",
    "gcn_layer_num =3\n",
    "learning_rate =0.01\n",
    "k_size = 50# [50,100,150,200,250,300,350,400,450,500]:\n",
    "alphas=[[1,0.1,0.1,0.1],\n",
    "        [1,0.2,0.2,0.2],\n",
    "        [1,0.3,0.3,0.3],\n",
    "        [1,0.4,0.4,0.4],\n",
    "        [1,0.5,0.5,0.5],\n",
    "        [1,0.6,0.6,0.6],\n",
    "        [1,0.7,0.7,0.7],\n",
    "        [1,0.8,0.8,0.8],\n",
    "        [1,0.9,0.9,0.9],\n",
    "        [1,1,1,1]]\n",
    "for alpha in alphas :\n",
    "    # model\n",
    "    model = gcn(40,32,k_size,gcn_layer_num,P,device,alpha)\n",
    "    # model = Linear1(48,512,500,P,device)\n",
    "    model.double().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.99),weight_decay=weight_decay)\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=40, verbose=False,\n",
    "                                                           min_lr=1e-6)\n",
    "    # training \n",
    "    model.train()\n",
    "    for epoch in (range(max_epoch)):\n",
    "        optimizer.zero_grad()\n",
    "        start=time.time()\n",
    "        out = model(torch_x1,torch_x2)\n",
    "        loss = criterion(out, train_val_labels)\n",
    "        train_loss= loss[train_flag==1].mean()\n",
    "        val_loss= loss[train_flag==0].mean()\n",
    "        train_loss.backward()\n",
    "        #     print(train_loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        predict = out[:,1].detach().to('cpu').numpy()\n",
    "        acc_predict= out.argmax(dim=1).detach().to('cpu').numpy()\n",
    "        np_train_labels = train_val_labels.detach().to('cpu').numpy()\n",
    "\n",
    "        auc_train= roc_auc_score(y_true=np_train_labels[train_flag.numpy()==1],y_score=predict[train_flag.numpy()==1])\n",
    "        auc_val =roc_auc_score(y_true=np_train_labels[train_flag.numpy()==0],y_score=predict[train_flag.numpy()==0] )\n",
    "\n",
    "        acc_train = accuracy_score(np_train_labels[train_flag.numpy()==1],acc_predict[train_flag.numpy()==1])\n",
    "        acc_val = accuracy_score(np_train_labels[train_flag.numpy()==0],acc_predict[train_flag.numpy()==0])\n",
    "\n",
    "        scheduler.step(auc_val)\n",
    "        #                     print('train: [%4d/ %5d], train loss: %.6f, val loss : %.6f, train auc: %.6f, val auc: %.6f, train acc: %.6f, val acc: %.6f,  time: %f s' %\n",
    "        #                               (epoch + 1, max_epoch, train_loss.item(), val_loss.item(), auc_train,auc_val,acc_train,acc_val, time.time() - start))\n",
    "#         record[weight_decay][gcn_layer_num][learning_rate][k_size]=[max(record[weight_decay][gcn_layer_num][learning_rate][k_size][0],auc_val),\n",
    "#                                                                     max(record[weight_decay][gcn_layer_num][learning_rate][k_size][1],acc_val)]\n",
    "        record =[max(record[0],auc_val),max(record[1],acc_val)]\n",
    "\n",
    "        json.dump(record,open('parameter_select.json','w'))\n",
    "    print(alpha)\n",
    "    print('auc:{},acc:{}'.format(record[0],record[1]))\n",
    "#     print ('weight_decay:{}, gcn_layer_num:{},k_size:{},learning_rate:{},auc:{},acc:{}'.format(weight_decay,gcn_layer_num,k_size,learning_rate,\\\n",
    "#                                                                                                record[weight_decay][gcn_layer_num][learning_rate][k_size][0],\\\n",
    "#                                                                                               record[weight_decay][gcn_layer_num][learning_rate][k_size][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0.1, 0.1, 0.1]\n",
      "auc:0.9322245442935098,acc:0.8778625954198473\n",
      "[1, 0.2, 0.2, 0.2]\n",
      "auc:0.9322245442935098,acc:0.8778625954198473\n",
      "[1, 0.3, 0.3, 0.3]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9fd95c4a81cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_flag\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_flag\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m#     print(train_loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python37/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/python37/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "record = [0,0]\n",
    "weight_decay = 0.001\n",
    "gcn_layer_num =3\n",
    "learning_rate =0.01\n",
    "k_size = 50# [50,100,150,200,250,300,350,400,450,500]:\n",
    "alphas=[[1,0.1,0.1,0.1],\n",
    "        [1,0.2,0.2,0.2],\n",
    "        [1,0.3,0.3,0.3],\n",
    "        [1,0.4,0.4,0.4],\n",
    "        [1,0.5,0.5,0.5],\n",
    "        [1,0.6,0.6,0.6],\n",
    "        [1,0.7,0.7,0.7],\n",
    "        [1,0.8,0.8,0.8],\n",
    "        [1,0.9,0.9,0.9],\n",
    "        [1,1,1,1]]\n",
    "for alpha in alphas :\n",
    "    # model\n",
    "    model = gcn(40,32,k_size,gcn_layer_num,P,device,alpha)\n",
    "    # model = Linear1(48,512,500,P,device)\n",
    "    model.double().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.99),weight_decay=weight_decay)\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=40, verbose=False,\n",
    "                                                           min_lr=1e-6)\n",
    "    # training \n",
    "    model.train()\n",
    "    print(model.alpha)\n",
    "    for epoch in (range(max_epoch)):\n",
    "        optimizer.zero_grad()\n",
    "        start=time.time()\n",
    "        out = model(torch_x1,torch_x2)\n",
    "        loss = criterion(out, train_val_labels)\n",
    "        train_loss= loss[train_flag==1].mean()\n",
    "        val_loss= loss[train_flag==0].mean()\n",
    "        train_loss.backward()\n",
    "        #     print(train_loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        predict = out[:,1].detach().to('cpu').numpy()\n",
    "        acc_predict= out.argmax(dim=1).detach().to('cpu').numpy()\n",
    "        np_train_labels = train_val_labels.detach().to('cpu').numpy()\n",
    "\n",
    "        auc_train= roc_auc_score(y_true=np_train_labels[train_flag.numpy()==1],y_score=predict[train_flag.numpy()==1])\n",
    "        auc_val =roc_auc_score(y_true=np_train_labels[train_flag.numpy()==0],y_score=predict[train_flag.numpy()==0] )\n",
    "\n",
    "        acc_train = accuracy_score(np_train_labels[train_flag.numpy()==1],acc_predict[train_flag.numpy()==1])\n",
    "        acc_val = accuracy_score(np_train_labels[train_flag.numpy()==0],acc_predict[train_flag.numpy()==0])\n",
    "\n",
    "        scheduler.step(auc_val)\n",
    "        #                     print('train: [%4d/ %5d], train loss: %.6f, val loss : %.6f, train auc: %.6f, val auc: %.6f, train acc: %.6f, val acc: %.6f,  time: %f s' %\n",
    "        #                               (epoch + 1, max_epoch, train_loss.item(), val_loss.item(), auc_train,auc_val,acc_train,acc_val, time.time() - start))\n",
    "#         record[weight_decay][gcn_layer_num][learning_rate][k_size]=[max(record[weight_decay][gcn_layer_num][learning_rate][k_size][0],auc_val),\n",
    "#                                                                     max(record[weight_decay][gcn_layer_num][learning_rate][k_size][1],acc_val)]\n",
    "        record =[max(record[0],auc_val),max(record[1],acc_val)]\n",
    "\n",
    "        json.dump(record,open('parameter_select.json','w'))\n",
    "    \n",
    "    print('auc:{},acc:{}'.format(record[0],record[1]))\n",
    "#     print ('weight_decay:{}, gcn_layer_num:{},k_size:{},learning_rate:{},auc:{},acc:{}'.format(weight_decay,gcn_layer_num,k_size,learning_rate,\\\n",
    "#                                                                                                record[weight_decay][gcn_layer_num][learning_rate][k_size][0],\\\n",
    "#                                                                                               record[weight_decay][gcn_layer_num][learning_rate][k_size][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0.1, 0.1, 0.1]\n",
      "auc:0.9292058516196448,acc:0.8676844783715013\n",
      "[1, 0.2, 0.2, 0.2]\n",
      "auc:0.9292058516196448,acc:0.8778625954198473\n",
      "[1, 0.3, 0.3, 0.3]\n",
      "auc:0.9292058516196448,acc:0.8778625954198473\n",
      "[1, 0.4, 0.4, 0.4]\n",
      "auc:0.9302507836990596,acc:0.8804071246819338\n",
      "[1, 0.5, 0.5, 0.5]\n",
      "auc:0.9302507836990596,acc:0.8854961832061069\n",
      "[1, 0.6, 0.6, 0.6]\n",
      "auc:0.9302507836990596,acc:0.8854961832061069\n",
      "[1, 0.7, 0.7, 0.7]\n",
      "auc:0.9302507836990596,acc:0.8854961832061069\n",
      "[1, 0.8, 0.8, 0.8]\n",
      "auc:0.9302507836990596,acc:0.8854961832061069\n",
      "[1, 0.9, 0.9, 0.9]\n",
      "auc:0.9302507836990596,acc:0.8854961832061069\n",
      "[1, 1, 1, 1]\n",
      "auc:0.9302507836990596,acc:0.8854961832061069\n"
     ]
    }
   ],
   "source": [
    "record = [0,0]\n",
    "weight_decay = 0.001\n",
    "gcn_layer_num =0\n",
    "learning_rate =0.01\n",
    "k_size = 50# [50,100,150,200,250,300,350,400,450,500]:\n",
    "alphas=[[1,0.1,0.1,0.1],\n",
    "        [1,0.2,0.2,0.2],\n",
    "        [1,0.3,0.3,0.3],\n",
    "        [1,0.4,0.4,0.4],\n",
    "        [1,0.5,0.5,0.5],\n",
    "        [1,0.6,0.6,0.6],\n",
    "        [1,0.7,0.7,0.7],\n",
    "        [1,0.8,0.8,0.8],\n",
    "        [1,0.9,0.9,0.9],\n",
    "        [1,1,1,1]]\n",
    "for alpha in alphas :\n",
    "    # model\n",
    "    model = gcn(40,32,k_size,gcn_layer_num,P,device,alpha)\n",
    "    # model = Linear1(48,512,500,P,device)\n",
    "    model.double().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.99),weight_decay=weight_decay)\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=40, verbose=False,\n",
    "                                                           min_lr=1e-6)\n",
    "    # training \n",
    "    model.train()\n",
    "    print(model.alpha)\n",
    "    for epoch in (range(max_epoch)):\n",
    "        optimizer.zero_grad()\n",
    "        start=time.time()\n",
    "        out = model(torch_x1,torch_x2)\n",
    "        loss = criterion(out, train_val_labels)\n",
    "        train_loss= loss[train_flag==1].mean()\n",
    "        val_loss= loss[train_flag==0].mean()\n",
    "        train_loss.backward()\n",
    "        #     print(train_loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        predict = out[:,1].detach().to('cpu').numpy()\n",
    "        acc_predict= out.argmax(dim=1).detach().to('cpu').numpy()\n",
    "        np_train_labels = train_val_labels.detach().to('cpu').numpy()\n",
    "\n",
    "        auc_train= roc_auc_score(y_true=np_train_labels[train_flag.numpy()==1],y_score=predict[train_flag.numpy()==1])\n",
    "        auc_val =roc_auc_score(y_true=np_train_labels[train_flag.numpy()==0],y_score=predict[train_flag.numpy()==0] )\n",
    "\n",
    "        acc_train = accuracy_score(np_train_labels[train_flag.numpy()==1],acc_predict[train_flag.numpy()==1])\n",
    "        acc_val = accuracy_score(np_train_labels[train_flag.numpy()==0],acc_predict[train_flag.numpy()==0])\n",
    "\n",
    "        scheduler.step(auc_val)\n",
    "        #                     print('train: [%4d/ %5d], train loss: %.6f, val loss : %.6f, train auc: %.6f, val auc: %.6f, train acc: %.6f, val acc: %.6f,  time: %f s' %\n",
    "        #                               (epoch + 1, max_epoch, train_loss.item(), val_loss.item(), auc_train,auc_val,acc_train,acc_val, time.time() - start))\n",
    "#         record[weight_decay][gcn_layer_num][learning_rate][k_size]=[max(record[weight_decay][gcn_layer_num][learning_rate][k_size][0],auc_val),\n",
    "#                                                                     max(record[weight_decay][gcn_layer_num][learning_rate][k_size][1],acc_val)]\n",
    "        record =[max(record[0],auc_val),max(record[1],acc_val)]\n",
    "\n",
    "        json.dump(record,open('parameter_select.json','w'))\n",
    "    \n",
    "    print('auc:{},acc:{}'.format(record[0],record[1]))\n",
    "#     print ('weight_decay:{}, gcn_layer_num:{},k_size:{},learning_rate:{},auc:{},acc:{}'.format(weight_decay,gcn_layer_num,k_size,learning_rate,\\\n",
    "#                                                                                                record[weight_decay][gcn_layer_num][learning_rate][k_size][0],\\\n",
    "#                                                                                               record[weight_decay][gcn_layer_num][learning_rate][k_size][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
